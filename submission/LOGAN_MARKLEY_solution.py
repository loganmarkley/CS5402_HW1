import pandas as pd
import ast
import numpy as np
import wfdb
from typing import Dict

"""PLEASE RENAME your solution TO FIRSTNAME_LASTNAME_solution.py"""

# Step-1, part 1
def parse_ptbxl_data()->pd.DataFrame:
    """
    Use pandas for this task.
    Implement this function to parse the ptbxl samples. It should
    return a dataframe with the filename_lr and diagnostic_class column.
    The filename_lr is the filename of the low-resolution ECG signal, and
    the diagnostic_class is generated by converting the scp_codes
    in the ptbxl database to diagnostic_class in the scp_statements database.
    There are 5 classes in total: [NORM, MI, STTC, CD, HYP].
    
    Since we want to simulate using the dataset to train a model, the class needs to have 
    at least one class, and any empty entries should be removed. 
    
    The end result should look like this:
    ecg_id filename_lr diagnostic_class
    1 records100/xxxxx/xxxxxx_lr  ['HYP']
    2 records100/xxxxx/xxxxxx_lr  ['MI']
    3 records100/xxxxx/xxxxxx_lr  ['MI, STTC']
    ...
    """
    #TODO: Implement this function
    
    ptbxl_df = pd.read_csv('ptbxl_sample.csv')
    scp_statements_df = pd.read_csv('scp_statements.csv')

    scp_to_diagnostic = scp_statements_df.set_index('Unnamed: 0')['diagnostic_class'].to_dict()

    def map_scp_to_diagnostic(scp_codes):
        scp_dict = ast.literal_eval(scp_codes)
        diagnostic_classes = {'NORM', 'MI', 'STTC', 'CD', 'HYP'}
        return list(set(scp_to_diagnostic[code] for code in scp_dict) & diagnostic_classes)

    ptbxl_df['diagnostic_class'] = ptbxl_df['scp_codes'].apply(map_scp_to_diagnostic)

    result_df = ptbxl_df[ptbxl_df['diagnostic_class'].apply(len) > 0]
    result_df = result_df[['ecg_id', 'filename_lr', 'diagnostic_class']]

    return result_df


# Step-1, part 2
def create_dataset(df: pd.DataFrame) -> tuple[np.ndarray, np.ndarray]:
 
    """
    Use numpy and wfdb for this task.
    The data are located in /records100/, and you can read them like this:
    signal, _ = wfdb.rdsamp(filepath), where signal is a numpy array that
    should have shape [signal_length(1000), num_channels(12)].
    Implement this function to create a dataset from the dataframe df, which should be the output of 
    parse_ptbxl_data. 

    Convert the textual class labels into one-hot encoding. For example, using the label order [NORM, MI, STTC, CD, HYP],
    an ECG signal with labels [HYP, MI, STTC] would be converted to [0, 1, 1, 0, 1].

    Return two numpy arrays:
    - data_x: array should contain the ECG data with shape [num_samples, signal_length(1000), num_channels(12)].
    - data_y: array should contain the labels with shape   [num_samples, num_classes(5)].
    """
    #TODO: Implement this function

    # you can read the raw ECG signal with this function:
    # ECG_signal, _ = wfdb.rdsamp("filename_lr")

    label_order = ['NORM', 'MI', 'STTC', 'CD', 'HYP']
    num_samples = len(df)
    data_x = np.zeros((num_samples, 1000, 12))
    data_y = np.zeros((num_samples, len(label_order)))

    for i, (filepath, labels) in enumerate(zip(df['filename_lr'], df['diagnostic_class'])):
        ecg_signal, _ = wfdb.rdsamp(filepath)
        data_x[i] = ecg_signal[:1000, :12]
        data_y[i] = [1 if label in labels else 0 for label in label_order]

    return data_x, data_y

# Step-2:
def data_preprocessing(data_x: np.ndarray, data_y: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
    """
    Perform data preprocessing: 
    - Check for missing values (or N/A), anomalies, and outliers. 
        - Fill missing values with the average of adjacent points in the same channel. 
        - Replace outliers using the 97th percentile (np.percentile(x, 97)). 
    - Normalize each channel with the equation: (x - xmin)/(xmax - xmin).
        - xmax: represents the maximum value of a channel
        - xmin: represents the minimum value of the channel.
    After normalization, the values will be scaled to range from 0 to 1.
    """

    #TODO: Implement this function.

    for i in range(data_x.shape[0]):
        for j in range(data_x.shape[2]):  # Iterate over channels
            channel = data_x[i, :, j]
            
            # Handle missing values
            mask = np.isnan(channel)
            channel[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), channel[~mask])
            
            # Handle outliers
            lower_bound, upper_bound = np.percentile(channel, [3, 97])
            channel = np.clip(channel, lower_bound, upper_bound)
            
            # Normalize
            xmin, xmax = np.min(channel), np.max(channel)
            if xmax > xmin:
                channel = (channel - xmin) / (xmax - xmin)
            else:
                channel = np.zeros_like(channel)
            
            data_x[i, :, j] = channel

    return data_x, data_y


# Step-3
def split_data(data_x: np.ndarray, data_y: np.ndarray) -> tuple[Dict[str, np.ndarray], Dict[str, np.ndarray], Dict[str, np.ndarray]]:
    """
    Implement this function to split the dataset into train, test, and validation sets at a 7:2:1 ratio.
    """
    #TODO: Implement this function
    
    # train_dataset = {"data_x": np.ndarray,
    #                  "data_y": np.ndarray}
    # val_dataset   = {"data_x": np.ndarray,
    #                  "data_y": np.ndarray}
    # test_dataset  = {"data_x": np.ndarray,
    #                  "data_y": np.ndarray}
    #return train_dataset, val_dataset, test_dataset.

    num_samples = data_x.shape[0]
    
    train_end_index = int(0.7 * num_samples)
    val_end_index = int(0.9 * num_samples)
    
    train_dataset = {"data_x": data_x[:train_end_index],
                     "data_y": data_y[:train_end_index]}
    val_dataset   = {"data_x": data_x[train_end_index:val_end_index],
                     "data_y": data_y[train_end_index:val_end_index]}
    test_dataset  = {"data_x": data_x[val_end_index:],
                     "data_y": data_y[val_end_index:]}
    
    return train_dataset, val_dataset, test_dataset